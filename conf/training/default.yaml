name: default
steps: 10000
batch_size: 16
seq_len: 256

# Optimizer
optimizer: adamw
lr: 3e-4
weight_decay: 0.1
betas: [0.9, 0.95]

# Learning rate schedule
lr_schedule: cosine
warmup_steps: 100
min_lr_ratio: 0.1

# Gradient handling
grad_clip: 1.0
grad_accum_steps: 1

# Mixed precision
mixed_precision: true

# Early stopping (0 = disabled)
early_stopping_patience: 0
early_stopping_min_delta: 0.0
