name: 100m
architecture: llama
dim: 1024
n_layers: 16
n_heads: 16
n_kv_heads: 4  # GQA: 4 KV heads for 16 query heads
dropout: 0.1
max_seq_len: 1024
gradient_checkpointing: true  # Save memory
attention_op: flash  # Use flash attention if available
